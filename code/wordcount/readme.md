# Hadoop MapReduce å®éªŒï¼šReduce å¯åŠ¨æ—¶æœºï¼ˆSlowstartï¼‰è°ƒä¼˜

æœ¬é¡¹ç›®ç”¨äº"å¤§è§„æ¨¡æ•°æ®å¤„ç†ç³»ç»Ÿ"è¯¾ç¨‹å®éªŒï¼šé€šè¿‡è°ƒèŠ‚å‚æ•° `mapreduce.job.reduce.slowstart.completedmaps` è§‚å¯Ÿ Reduce ä»»åŠ¡å¯åŠ¨æ—¶æœºå¯¹ä½œä¸šå¹¶è¡Œåº¦ã€Shuffle é‡å åº¦ã€èµ„æºåˆ©ç”¨ä¸æ€»è€—æ—¶çš„å½±å“ã€‚  
è¿è¡Œç¯å¢ƒåŸºäº Hadoop 3.2.4ï¼Œç¤ºä¾‹ç¨‹åºä¸ºç®€å•è¯é¢‘ç»Ÿè®¡ï¼Œå¯ç¨³å®šå¤ç°å®éªŒç°è±¡ã€‚

---

## ğŸ—‚ï¸ é¡¹ç›®è„šæœ¬æ¦‚è§ˆ

### æ ¸å¿ƒè„šæœ¬æ–‡ä»¶ç»“æ„
```
/home/ecs-user/MRApplication/reduce-startup/
â”œâ”€â”€ scripts/                           # æ‰€æœ‰è„šæœ¬ç»Ÿä¸€å­˜æ”¾ç›®å½•
â”‚   â”œâ”€â”€ generate_data.py              # ğŸ†• ç»Ÿä¸€æ•°æ®ç”Ÿæˆå™¨ï¼ˆæ”¯æŒä»»æ„å¤§å°ï¼‰
â”‚   â”œâ”€â”€ collect_metrics.sh            # ğŸ†• å¤šèŠ‚ç‚¹ç³»ç»ŸæŒ‡æ ‡æ”¶é›†ï¼ˆæ”¯æŒèŠ‚ç‚¹åå‚æ•°ï¼‰
â”‚   â”œâ”€â”€ merge_node_metrics.sh         # ğŸ†• å¤šèŠ‚ç‚¹æ•°æ®åˆå¹¶å·¥å…·
â”‚   â”œâ”€â”€ monitor_job.sh                # âœ… å•æ¬¡å®éªŒç›‘æµ‹ï¼ˆå·²ä¼˜åŒ–CSVè¾“å‡ºï¼‰
â”‚   â”œâ”€â”€ batch_experiment.sh           # âœ… æ‰¹é‡å®éªŒè„šæœ¬
â”‚   â”œâ”€â”€ process_metrics.sh            # æ•°æ®å¤„ç†è„šæœ¬
â”‚   â”œâ”€â”€ generate_report.sh            # åˆ†ææŠ¥å‘Šç”Ÿæˆè„šæœ¬
â”‚   â””â”€â”€ test_metrics_csv.sh           # CSVæ ¼å¼æµ‹è¯•è„šæœ¬
â”œâ”€â”€ run_scripts.sh                     # ğŸ†• ä¾¿æ·è„šæœ¬æ‰§è¡Œå™¨
â”œâ”€â”€ docs/                             
â”‚   â””â”€â”€ MULTI_NODE_USAGE.md           # ğŸ†• å¤šèŠ‚ç‚¹ä½¿ç”¨æŒ‡å—
â”œâ”€â”€ system_metrics/                    # ğŸ†• ç³»ç»Ÿæ—¶åºæŒ‡æ ‡æ•°æ®ï¼ˆå¸¦æ—¶é—´æˆ³ï¼Œä¸è¦†ç›–ï¼‰
â”‚   â”œâ”€â”€ {èŠ‚ç‚¹å}_{æ—¶é—´æˆ³}.csv          # å¦‚: iZbp17ue5tnwdnupp4di68Z_20251125_133131.csv
â”‚   â””â”€â”€ README.md                     # ç›®å½•è¯´æ˜æ–‡æ¡£
â””â”€â”€ metrics/                          # å®éªŒç»“æœæ•°æ®è¾“å‡ºç›®å½•
    â”œâ”€â”€ experiment_*.csv              # å•æ¬¡å®éªŒç»“æœ
    â”œâ”€â”€ batch_summary_*.csv           # æ‰¹é‡å®éªŒæ±‡æ€»
    â””â”€â”€ analysis_report_*.txt         # åˆ†ææŠ¥å‘Š
```

### ğŸš€ å¿«é€Ÿå¼€å§‹
```bash
# æŸ¥çœ‹æ‰€æœ‰å¯ç”¨è„šæœ¬
./run_scripts.sh

# ç”Ÿæˆæµ‹è¯•æ•°æ®
./run_scripts.sh generate_data.py 100  # ç”Ÿæˆ100MBæ•°æ®

# è¿è¡Œå•æ¬¡å®éªŒ
./run_scripts.sh monitor_job.sh 0.3

# è¿è¡Œæ‰¹é‡å®éªŒ
./run_scripts.sh batch_experiment.sh

# å¤šèŠ‚ç‚¹æ•°æ®æ”¶é›†ï¼ˆè¯¦è§docs/MULTI_NODE_USAGE.mdï¼‰
./run_scripts.sh collect_metrics.sh master 1
./run_scripts.sh merge_node_metrics.sh cluster_metrics.csv master.csv worker*.csv
```

---

## âœ… æ€§èƒ½ç›‘æµ‹ç³»ç»Ÿ

æœ¬é¡¹ç›®å·²é›†æˆå®Œæ•´çš„æ€§èƒ½ç›‘æµ‹ç³»ç»Ÿï¼Œæ”¯æŒå•èŠ‚ç‚¹å’Œå¤šèŠ‚ç‚¹Hadoopé›†ç¾¤çš„æ€§èƒ½æ•°æ®æ”¶é›†ï¼Œå¹¶å°†ç»“æœä¿å­˜ä¸ºæ ‡å‡†åŒ–CSVæ ¼å¼ã€‚

### ğŸ“Š æ•°æ®è¡¨æ ¼è§„èŒƒ

#### ğŸ• æ€§èƒ½ç›‘æµ‹è¡¨ï¼ˆæ—¶åºæ•°æ®ï¼‰- System Metrics CSV
ç”¨äºè®°å½•ç³»ç»Ÿèµ„æºçš„æ—¶åºå˜åŒ–ï¼Œæ–‡ä»¶å‘½åï¼š`{èŠ‚ç‚¹å}.csv`

| åˆ—å | æ•°æ®ç±»å‹ | å•ä½ | å«ä¹‰è¯´æ˜ |
|------|----------|------|----------|
| `node_name` | string | - | èŠ‚ç‚¹åç§°æ ‡è¯†ï¼ˆå¦‚masterã€worker01ç­‰ï¼‰ |
| `timestamp` | integer | seconds | Unixæ—¶é—´æˆ³ï¼Œæ•°æ®é‡‡é›†æ—¶åˆ» |
| `cpu_percent` | float | % | CPUæ•´ä½“ä½¿ç”¨ç‡ï¼ˆ0-100ï¼‰ |
| `memory_used_mb` | integer | MB | å·²ä½¿ç”¨å†…å­˜å¤§å° |
| `memory_total_mb` | integer | MB | ç³»ç»Ÿæ€»å†…å­˜å¤§å° |
| `memory_percent` | float | % | å†…å­˜ä½¿ç”¨ç‡ï¼ˆ0-100ï¼‰ |
| `load_avg` | float | - | ç³»ç»Ÿ1åˆ†é’Ÿå¹³å‡è´Ÿè½½ |
| `disk_reads` | integer | ops | ç´¯è®¡ç£ç›˜è¯»æ“ä½œæ¬¡æ•° |
| `disk_writes` | integer | ops | ç´¯è®¡ç£ç›˜å†™æ“ä½œæ¬¡æ•° |
| `network_rx_mb` | float | MB | ç´¯è®¡ç½‘ç»œæ¥æ”¶æµé‡ |
| `network_tx_mb` | float | MB | ç´¯è®¡ç½‘ç»œå‘é€æµé‡ |
| `java_cpu_percent` | float | % | Hadoop Javaè¿›ç¨‹CPUä½¿ç”¨ç‡ |
| `java_memory_percent` | float | % | Hadoop Javaè¿›ç¨‹å†…å­˜ä½¿ç”¨ç‡ |
| `java_processes` | integer | count | æ´»è·ƒçš„Hadoopè¿›ç¨‹æ•°é‡ |

**ç¤ºä¾‹æ•°æ®ï¼š**
```csv
node_name,timestamp,cpu_percent,memory_used_mb,memory_total_mb,memory_percent,load_avg,disk_reads,disk_writes,network_rx_mb,network_tx_mb,java_cpu_percent,java_memory_percent,java_processes
master,1764038886,3.2,1414,7658,18.5,0.06,0,0,0,0,0,0,0
worker01,1764038887,25.4,2048,4096,50.0,1.25,145,67,12.5,8.3,18.7,15.2,3
```

#### ğŸ“ˆ å®éªŒç»“æœè¡¨ï¼ˆå¯¹æ¯”æ•°æ®ï¼‰- Experiment Results CSV
ç”¨äºè®°å½•ä¸åŒå®éªŒé…ç½®çš„ç»“æœå¯¹æ¯”ï¼Œæ–‡ä»¶å‘½åï¼š`experiment_{å®éªŒID}_slowstart_{å€¼}.csv`

| åˆ—å | æ•°æ®ç±»å‹ | å•ä½ | å«ä¹‰è¯´æ˜ |
|------|----------|------|----------|
| `experiment_id` | string | - | å®éªŒå”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆé€šå¸¸ä¸ºæ—¶é—´æˆ³ï¼‰ |
| `slowstart_value` | float | - | MapReduceæ…¢å¯åŠ¨å‚æ•°å€¼ï¼ˆ0.0-1.0ï¼‰ |
| `start_time` | integer | seconds | å®éªŒå¼€å§‹æ—¶é—´æˆ³ |
| `end_time` | integer | seconds | å®éªŒç»“æŸæ—¶é—´æˆ³ |
| `total_time_sec` | integer | seconds | ä½œä¸šæ€»æ‰§è¡Œæ—¶é—´ |
| `avg_cpu_percent` | float | % | å®éªŒæœŸé—´å¹³å‡CPUä½¿ç”¨ç‡ |
| `max_cpu_percent` | float | % | å®éªŒæœŸé—´æœ€å¤§CPUä½¿ç”¨ç‡ |
| `avg_memory_mb` | float | MB | å®éªŒæœŸé—´å¹³å‡å†…å­˜ä½¿ç”¨é‡ |
| `max_memory_mb` | float | MB | å®éªŒæœŸé—´æœ€å¤§å†…å­˜ä½¿ç”¨é‡ |
| `avg_load` | float | - | å®éªŒæœŸé—´å¹³å‡ç³»ç»Ÿè´Ÿè½½ |
| `max_load` | float | - | å®éªŒæœŸé—´æœ€å¤§ç³»ç»Ÿè´Ÿè½½ |
| `bytes_read` | long | bytes | ä½œä¸šè¯»å–çš„æ€»æ•°æ®é‡ |
| `bytes_written` | long | bytes | ä½œä¸šå†™å…¥çš„æ€»æ•°æ®é‡ |
| `map_tasks` | integer | count | Mapä»»åŠ¡æ€»æ•° |
| `reduce_tasks` | integer | count | Reduceä»»åŠ¡æ€»æ•° |
| `job_status` | string | - | ä½œä¸šæ‰§è¡ŒçŠ¶æ€ï¼ˆSUCCESS/FAILEDï¼‰ |

**ç¤ºä¾‹æ•°æ®ï¼š**
```csv
experiment_id,slowstart_value,start_time,end_time,total_time_sec,avg_cpu_percent,max_cpu_percent,avg_memory_mb,max_memory_mb,avg_load,max_load,bytes_read,bytes_written,map_tasks,reduce_tasks,job_status
20231124_143000,0.3,1700812200,1700812220,20,45.2,78.5,1024,1456,0.85,2.14,1073741824,52428800,8,2,SUCCESS
20231124_143500,0.7,1700812500,1700812528,28,42.1,68.3,998,1289,0.72,1.89,1073741824,52428800,8,2,SUCCESS
```

### ğŸ“‹ æ ‡å‡†è§„èŒƒï¼ˆä¾›å…¶ä»–å­é¡¹ç›®å‚è€ƒï¼‰

#### 1. æ–‡ä»¶å‘½åè§„èŒƒ
- **ç³»ç»ŸæŒ‡æ ‡** (æ›´æ–°äº2025-11-25): `system_metrics/{èŠ‚ç‚¹å}_{æ—¶é—´æˆ³}.csv`
  - æ–°æ ¼å¼ç¤ºä¾‹ï¼š`system_metrics/iZbp17ue5tnwdnupp4di68Z_20251125_133131.csv`
  - æ—§æ ¼å¼ï¼š`{èŠ‚ç‚¹å}.csv` (å·²åºŸå¼ƒï¼Œä¼šè¢«è¦†ç›–)
  - **é‡è¦æ”¹è¿›**ï¼šæ¯æ¬¡å®éªŒç”Ÿæˆç‹¬ç«‹æ–‡ä»¶ï¼Œé¿å…å†å²æ•°æ®è¢«è¦†ç›–
- **å®éªŒç»“æœ**: `metrics/experiment_{å®éªŒID}_slowstart_{å€¼}.csv`
- **æ‰¹é‡æ±‡æ€»**: `metrics/batch_summary_{æ—¶é—´æˆ³}.csv`
- **æ—¶é—´æˆ³**: ä½¿ç”¨Unixæ—¶é—´æˆ³ï¼ˆç§’çº§ç²¾åº¦ï¼‰
- **ç™¾åˆ†æ¯”**: ä½¿ç”¨0-100èŒƒå›´çš„æµ®ç‚¹æ•°
- **å†…å­˜/å­˜å‚¨**: ç»Ÿä¸€ä½¿ç”¨MBæˆ–byteså•ä½
- **å­—ç¬¦ä¸²**: ä½¿ç”¨è‹±æ–‡ï¼Œé¿å…ç‰¹æ®Šå­—ç¬¦
- **å¸ƒå°”å€¼**: ä½¿ç”¨SUCCESS/FAILEDç­‰æ˜ç¡®å­—ç¬¦ä¸²

#### 3. CSVæ–‡ä»¶è¦æ±‚
- **ç¼–ç **: UTF-8
- **åˆ†éš”ç¬¦**: è‹±æ–‡é€—å·ï¼ˆ,ï¼‰
- **è¡¨å¤´**: ç¬¬ä¸€è¡Œå¿…é¡»ä¸ºåˆ—å
- **æ— ç©ºè¡Œ**: æ•°æ®è¡Œä¹‹é—´ä¸å…è®¸ç©ºè¡Œ
- **æ•°å€¼ç²¾åº¦**: æµ®ç‚¹æ•°ä¿ç•™1-2ä½å°æ•°

#### 4. å¤šèŠ‚ç‚¹æ•°æ®åˆå¹¶
- æ‰€æœ‰èŠ‚ç‚¹çš„CSVæ–‡ä»¶å¿…é¡»å…·æœ‰ç›¸åŒçš„åˆ—ç»“æ„
- ç¬¬ä¸€åˆ—å¿…é¡»ä¸º`node_name`ä»¥ä¾¿åŒºåˆ†æ•°æ®æ¥æº
- åˆå¹¶åæŒ‰`timestamp`æ’åºä¾¿äºæ—¶åºåˆ†æ
- ä½¿ç”¨`scripts/merge_node_metrics.sh`è¿›è¡Œæ ‡å‡†åŒ–åˆå¹¶

### ï¿½ ä½¿ç”¨æ–¹æ³•

#### 1. å•æ¬¡å®éªŒç›‘æµ‹
```bash
# åŸºæœ¬ç”¨æ³• - ä½¿ç”¨é»˜è®¤å‚æ•°
./monitor_job.sh

# æŒ‡å®šslowstartå€¼
./monitor_job.sh 0.3

# å®Œæ•´å‚æ•°
./monitor_job.sh 0.3 /mr_input /mr_output_03 experiment_001
```

**å‚æ•°è¯´æ˜:**
- `slowstart_value`: Reduceæ…¢å¯åŠ¨å€¼ (0.1-1.0)
- `input_path`: HDFSè¾“å…¥è·¯å¾„ (é»˜è®¤: /mr_input)
- `output_path`: HDFSè¾“å‡ºè·¯å¾„ (é»˜è®¤: /mr_output)
- `experiment_id`: å®éªŒæ ‡è¯†ç¬¦ (é»˜è®¤: è‡ªåŠ¨ç”Ÿæˆæ—¶é—´æˆ³)

#### 2. æ‰¹é‡å®éªŒ
```bash
# è¿è¡Œé¢„è®¾çš„slowstartå€¼ (0.1, 0.3, 0.5, 0.7, 1.0)
./batch_experiment.sh

# æŒ‡å®šè¾“å…¥è¾“å‡ºè·¯å¾„
./batch_experiment.sh /mr_input /mr_output_batch
```

#### 3. æ‰‹åŠ¨è¿è¡Œç³»ç»Ÿç›‘æµ‹
```bash
# åå°ç›‘æµ‹ç³»ç»Ÿèµ„æºï¼Œæ¯ç§’é‡‡é›†ä¸€æ¬¡
./collect_metrics.sh system_metrics.tmp &

# åœæ­¢ç›‘æµ‹
kill %1  # æˆ–ä½¿ç”¨å…·ä½“çš„PID
```

### ğŸ“ è¾“å‡ºæ–‡ä»¶è¯´æ˜

#### CSVæ–‡ä»¶æ ¼å¼

**1. å®éªŒç»“æœCSV** (`experiment_*.csv`)
```csv
experiment_id,slowstart_value,start_time,end_time,total_time_sec,
avg_cpu_percent,max_cpu_percent,avg_memory_mb,max_memory_mb,
avg_load,max_load,bytes_read,bytes_written,map_tasks,reduce_tasks,job_status
```

**2. ä»»åŠ¡æ—¶é—´çº¿CSV** (`*_timeline.csv`) ğŸ†•
è®°å½•æ¯ä¸ªMap/Reduceä»»åŠ¡çš„å¯åŠ¨å’Œå®Œæˆæ—¶é—´ï¼ˆåŸå§‹æ•°æ®ï¼‰
```csv
experiment_id,slowstart_value,task_id,task_type,start_time,finish_time,elapsed_sec,
shuffle_finish_time,merge_finish_time,reduce_finish_time
```

**3. æ—¶é—´çº¿æ±‡æ€»CSV** (`*_timeline_summary.csv`) ğŸ†•
ç»Ÿè®¡Map/Reduceçš„å¹¶è¡Œæ‰§è¡Œæƒ…å†µ
```csv
experiment_id,slowstart_value,num_map_tasks,num_reduce_tasks,
map_start_time,map_end_time,map_duration_sec,
reduce_start_time,reduce_end_time,reduce_duration_sec,
overlap_duration_sec,reduce_start_at_map_pct,
total_time_sec,time_saved_sec,parallel_efficiency_pct
```

#### æ–‡ä»¶ç»“æ„
```
metrics/
â”œâ”€â”€ experiment_<id>_slowstart_<value>.csv           # å•æ¬¡å®éªŒç»“æœ
â”œâ”€â”€ <id>_slowstart_<value>_timeline.csv             # ğŸ†• ä»»åŠ¡æ—¶é—´çº¿ï¼ˆåŸå§‹æ•°æ®ï¼‰
â”œâ”€â”€ <id>_slowstart_<value>_timeline_summary.csv     # ğŸ†• æ—¶é—´çº¿ç»Ÿè®¡æ±‡æ€»
â”œâ”€â”€ batch_summary_<timestamp>.csv                   # æ‰¹é‡å®éªŒæ±‡æ€»
â””â”€â”€ analysis_report_<timestamp>.txt                 # åˆ†ææŠ¥å‘Š
```

### ğŸ” è¾“å‡ºç¤ºä¾‹

#### å•æ¬¡å®éªŒè¾“å‡º
```bash
=== Hadoop MapReduce Performance Monitor ===
Experiment ID: 20231124_143000
Slowstart Value: 0.3
Input Path: /mr_input
Output Path: /mr_output_03

Starting system resource monitoring...
Updating slowstart value to 0.3...
Compiling project...
Starting Hadoop job at Mon Nov 24 14:30:05 CST 2023...
Job completed at Mon Nov 24 14:30:25 CST 2023
Total execution time: 20 seconds

=== Experiment Summary ===
Experiment ID: 20231124_143000
Slowstart Value: 0.3
Total Time: 20 seconds
Job Status: SUCCESS
Metrics saved to: metrics/experiment_20231124_143000_slowstart_0.3.csv
```

#### æ‰¹é‡å®éªŒåˆ†æè¡¨æ ¼
```
Slowstart | Total Time | Avg CPU | Max Memory | Status
----------|------------|---------|------------|--------
0.1       | 25s        | 45.2%   | 1024MB     | SUCCESS
0.3       | 20s        | 52.8%   | 1156MB     | SUCCESS
0.5       | 22s        | 48.1%   | 1089MB     | SUCCESS
0.7       | 24s        | 44.3%   | 998MB      | SUCCESS
1.0       | 28s        | 41.7%   | 945MB      | SUCCESS
```

---

## ğŸ“¦ å¤§æ•°æ®é›†ç”Ÿæˆå™¨

ä¸ºäº†è·å¾—æ›´æ˜æ˜¾çš„æ€§èƒ½å·®å¼‚ï¼Œé¡¹ç›®æä¾›äº†å¤§æ•°æ®é›†ç”Ÿæˆå·¥å…·ã€‚

### ğŸ¯ æ•°æ®é›†ç‰¹ç‚¹
- **å¯é…ç½®å¤§å°**: æ”¯æŒç”Ÿæˆä»»æ„å¤§å°çš„æ•°æ®é›† (é»˜è®¤1GB)
- **çœŸå®å†…å®¹**: åŒ…å«Hadoop/å¤§æ•°æ®ç›¸å…³è¯æ±‡ï¼Œæ¨¡æ‹ŸçœŸå®åœºæ™¯
- **å¤šæ–‡ä»¶åˆ†å¸ƒ**: è‡ªåŠ¨æ‹†åˆ†ä¸ºå¤šä¸ªæ–‡ä»¶ï¼Œä¾¿äºå¹¶è¡Œå¤„ç†
- **ç»“æ„åŒ–æ•°æ®**: 30%ç»“æ„åŒ–æ¨¡å¼ + 70%éšæœºå†…å®¹ï¼Œæä¾›ä¸°å¯Œçš„reduceæ“ä½œ

### ğŸ“‹ ä½¿ç”¨æ–¹æ³•

#### ç”Ÿæˆ1GBæ•°æ®é›† (æ¨è)
```bash
# åŸºæœ¬ç”¨æ³• - ç”Ÿæˆ1GBæ•°æ®é›†ï¼Œ4ä¸ªæ–‡ä»¶
./generate_dataset.py

# æŒ‡å®šå¤§å°å’Œæ–‡ä»¶æ•°
./generate_dataset.py --size 2.0 --files 8

# å®Œæ•´å‚æ•°
./generate_dataset.py --size 1.5 --files 6 --output input-custom --prefix dataset
```

#### å‚æ•°è¯´æ˜
- `--size`: æ•°æ®é›†å¤§å° (GB) (é»˜è®¤: 1.0)
- `--files`: æ–‡ä»¶æ•°é‡ (é»˜è®¤: 4)
- `--output`: è¾“å‡ºç›®å½• (é»˜è®¤: input-large)
- `--prefix`: æ–‡ä»¶å‰ç¼€ (é»˜è®¤: data)

#### ä¸Šä¼ åˆ°HDFS
```bash
# åˆ‡æ¢åˆ°æ•°æ®é›†ç›®å½•
cd input-large

# ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„ä¸Šä¼ è„šæœ¬cd
./upload_to_hdfs.sh

# æˆ–æŒ‡å®šHDFSè·¯å¾„
./upload_to_hdfs.sh /mr_input_1gb
```

### ğŸ”„ å®Œæ•´å®éªŒæµç¨‹

#### 1. ç”Ÿæˆå¤§æ•°æ®é›†
```bash
./generate_dataset.py --size 1.0 --files 4
cd input-large && ./upload_to_hdfs.sh /mr_input_large
```

#### 2. ä½¿ç”¨å¤§æ•°æ®é›†è¿›è¡Œç›‘æµ‹å®éªŒ
```bash
# å•æ¬¡å®éªŒ
./monitor_job.sh 0.3 /mr_input_large /mr_output_large_03
./monitor_job.sh 0.3 /mr_input_zg /mr_output_zg

# æ‰¹é‡å®éªŒ
./batch_experiment.sh /mr_input_large /mr_output_large
```

#### 3. é¢„æœŸæ•ˆæœ
ä½¿ç”¨1GBæ•°æ®é›†åï¼Œä½ åº”è¯¥èƒ½è§‚å¯Ÿåˆ°ï¼š
- **æ›´æ˜æ˜¾çš„æ€§èƒ½å·®å¼‚**: ä¸åŒslowstartå€¼çš„å½±å“æ›´åŠ æ˜¾è‘—
- **æ›´é•¿çš„æ‰§è¡Œæ—¶é—´**: ä¾¿äºè§‚å¯Ÿå„é˜¶æ®µçš„èµ„æºä½¿ç”¨æ¨¡å¼
- **æ›´å¤šMap/Reduceä»»åŠ¡**: æä¾›æ›´ä¸°å¯Œçš„å¹¶è¡Œåº¦åˆ†ææ•°æ®
- **æ›´çœŸå®çš„èµ„æºç«äº‰**: æ›´å¥½åœ°åæ˜ ç”Ÿäº§ç¯å¢ƒç‰¹å¾

## TODO
ç›®å‰ç‰ˆæœ¬å·²å®Œæˆæ ¸å¿ƒåŠŸèƒ½ï¼š

- âœ… æ€§èƒ½ç›‘æµ‹åŠè®°å½•(CSVæ ¼å¼)
- âœ… å¤§æ•°æ®é›†ç”Ÿæˆå·¥å…·
- ç»™å‡ºæ›´å¤šç¤ºä¾‹ç¨‹åº (å¾…å®ç°)

---

## âœ¨ ç›®æ ‡
- ç†è§£ Reduce æ…¢å¯åŠ¨æœºåˆ¶ï¼ˆSlowstartï¼‰
- æŒæ¡å‚æ•° `mapreduce.job.reduce.slowstart.completedmaps` çš„è°ƒä¼˜æ•ˆæœ
- é‡‡é›†ä¸åŒè®¾ç½®ä¸‹çš„ä½œä¸šæ‰§è¡Œæ—¶é—´ä¸é˜¶æ®µè¡Œä¸º
- åˆ†æ Map / Shuffle / Reduce çš„å¹¶è¡Œå…³ç³»åŠèµ„æºåˆ©ç”¨

---

## ğŸ— Hadoopå·¥ç¨‹é¡¹ç›®ç»“æ„
```
reduce-startup/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main/java/edu/example/mapreduce/
â”‚       â”œâ”€â”€ Main.java        # Job æäº¤ä¸å‚æ•°è®¾ç½®
â”‚       â”œâ”€â”€ MapperA.java     # Map å®ç°
â”‚       â””â”€â”€ ReducerA.java    # Reduce å®ç°
â”œâ”€â”€ pom.xml                  # Maven æ„å»º
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ§° ç¯å¢ƒä¾èµ–
| ç»„ä»¶ | ç‰ˆæœ¬å»ºè®® |
|------|----------|
| Hadoop | 3.2.4 |
| Java | OpenJDK 8|
| Maven | 3.6 |
| OS | Ubuntu 20.04 |

---

## âš™ï¸ Hadoop åŸºç¡€æ£€æŸ¥
```bash
hadoop version
```
ç¡®ä¿è¾“å‡ºä¸­ç‰ˆæœ¬ä¸º 3.2.4ã€‚

---

## ğŸ“‚ å‡†å¤‡è¾“å…¥æ•°æ®ï¼ˆä»…é¦–æ¬¡ï¼‰
```bash
hdfs dfs -mkdir -p /mr_input

echo "hello hadoop hello mapreduce" > data1.txt
echo "hello world mapreduce experiment" > data2.txt
hdfs dfs -put -f data1.txt data2.txt /mr_input
```

---

## ğŸ”§ ç¼–è¯‘æ‰“åŒ…
```bash
mvn clean package -DskipTests
```
ç”Ÿæˆ Fat JARï¼š
```
target/reduce-startup-1.0-SNAPSHOT-jar-with-dependencies.jar
```

---

## ğŸš€ è¿è¡Œç¤ºä¾‹
```bash
hadoop jar target/reduce-startup-1.0-SNAPSHOT-jar-with-dependencies.jar \
    /mr_input /mr_output_01
```
æŸ¥çœ‹ç»“æœï¼š
```bash
hdfs dfs -ls /mr_output_01
hdfs dfs -cat /mr_output_01/part-r-00000
```

---

## ğŸ§ª æ ¸å¿ƒå‚æ•°ï¼šReduce æ…¢å¯åŠ¨
åœ¨ `Main.java` ä¸­ï¼š
```java
conf.setFloat("mapreduce.job.reduce.slowstart.completedmaps", 0.3f);
```
å«ä¹‰ï¼šå½“æŒ‡å®šæ¯”ä¾‹çš„ Map å®Œæˆåå…è®¸è°ƒåº¦ Reduceï¼ˆè¿›å…¥ Shuffle / Fetchï¼‰ã€‚

æ¨èå®éªŒç»„åˆï¼š
| ç¼–å· | slowstart å€¼ | æè¿° |
|------|--------------|------|
| A1 | 0.1 | ææ—©å¯åŠ¨ï¼Œå¯èƒ½ç©ºè½¬ç­‰å¾… Map è¾“å‡º |
| A2 | 0.3 | é€‚åº¦æå‰ï¼Œå¢åŠ  Shuffle ä¸ Map é‡å  |
| A3 | 0.7 | åæ™šï¼ŒMap é›†ä¸­å èµ„æº |
| A4 | 1.0 | ä¸²è¡Œå€¾å‘ï¼ŒMap å…¨éƒ¨å®Œæˆåæ‰å¯åŠ¨ Reduce |

---

## ğŸ§ª å®éªŒæ­¥éª¤ï¼ˆå¯¹æ¯ä¸ªå€¼é‡å¤ï¼‰
1. ä¿®æ”¹å‚æ•°  
   ç¼–è¾‘ `Main.java`ï¼š
   ```java
   conf.setFloat("mapreduce.job.reduce.slowstart.completedmaps", 0.7f);
   ```
2. é‡æ–°æ‰“åŒ…  
   ```bash
   mvn clean package -DskipTests
   ```
3. é€‰æ‹©æ–°çš„è¾“å‡ºç›®å½•ï¼ˆé¿å…å·²å­˜åœ¨å¯¼è‡´å¤±è´¥ï¼‰  
   ```bash
   hdfs dfs -rm -r -f /mr_output_07
   ```
4. è¿è¡Œå¹¶è®¡æ—¶  
   ```bash
   time hadoop jar target/reduce-startup-1.0-SNAPSHOT-jar-with-dependencies.jar \
        /mr_input /mr_output_07
   ```
   è®°å½• `real` æ—¶é—´ã€‚
5. é‡‡é›†æŒ‡æ ‡  
   - æ§åˆ¶å°ï¼šMap / Reduce è¿›åº¦æ¡ã€Shuffle é˜¶æ®µå¼€å§‹æ—¶é—´  
   - YARN UI: `http://<ResourceManager>:8088` â†’ Application â†’ Attempts  
   - JobHistoryï¼ˆè‹¥å¼€å¯ï¼‰ï¼š`http://<HistoryServer>:19888/jobhistory`  
   - `mapreduce.task.io.sort.mb`/å¹¶è¡Œåº¦å¯è¾…åŠ©è§£é‡Šå·®å¼‚
6. æ•´ç†ç»“æœå…¥è¡¨ï¼ˆç¤ºä¾‹ï¼‰ï¼š

| slowstart | Reduce å®é™…å¯åŠ¨ç‚¹ (Map å®Œæˆ %) | Shuffle é‡å åº¦ | æ€»æ—¶é—´ (s) | è§‚å¯Ÿ |
|-----------|-------------------------------|----------------|-----------|------|
| 0.1 | ~10% | é«˜ | ? | Reduce æ—©ï¼Œå¯èƒ½æ— æ•°æ®ç©ºè½®è¯¢ |
| 0.3 | ~30% | ä¸­é«˜ | ? | å¸¸è§è¾ƒä¼˜æŠ˜ä¸­ |
| 0.7 | ~70% | ä½ | ? | èµ„æºå€¾å‘ Map |
| 1.0 | 100% | æœ€ä½ | ? | è¿‘ä¼¼ä¸²è¡Œ |

å¡«å†™ ? ä¸ºå®æµ‹å€¼ã€‚

---

## ğŸ“Š è¿›ä¸€æ­¥åˆ†æå»ºè®®
- å¯¹æ¯”å„é…ç½®ä¸‹ï¼š
  - Map é˜¶æ®µå¹³å‡ CPU åˆ©ç”¨ç‡ï¼ˆä½¿ç”¨ `top` / `yarn node -list` / ç›‘æ§ï¼‰
  - Shuffle Fetch ç­‰å¾…æ—¶é—´ï¼ˆReduce Task æ—¥å¿—ä¸­ FetchStarted vs FirstMapOutputFetchedï¼‰
  - Spill æ¬¡æ•°ä¸ Merge æ—¶é—´ï¼ˆMap Task æ—¥å¿—ï¼‰
- å¯å†™è„šæœ¬æ‰¹é‡è¿è¡Œï¼š
  ```bash
  for v in 0.1 0.3 0.7 1.0; do
    sed -i "s/slowstart.completedmaps\", [0-9.]\+/slowstart.completedmaps\", $v/" \
        src/main/java/edu/example/mapreduce/Main.java
    mvn -q package -DskipTests
    out=/mr_output_${v//./}
    hdfs dfs -rm -r -f $out
    echo "== slowstart = $v =="
    /usr/bin/time -f "%E" hadoop jar target/reduce-startup-1.0-SNAPSHOT-jar-with-dependencies.jar /mr_input $out
  done
  ```
  å°†æ—¶é—´æ±‡æ€»è‡³ CSVã€‚

---

## ğŸ›  å¸¸è§é—®é¢˜
| é—®é¢˜ | å¤„ç† |
|------|------|
| è¾“å‡ºç›®å½•å­˜åœ¨ | å…ˆ `hdfs dfs -rm -r -f /mr_output_xx` |
| ClassNotFound | ç¡®è®¤ä½¿ç”¨å¸¦ä¾èµ–çš„ JAR |
| æƒé™é”™è¯¯ | æ£€æŸ¥ HDFS ç›®å½• owner ä¸ `hadoop fs -chmod` |
| Reduce ä¸å¯åŠ¨ | slowstart=1.0 ç­‰å¾…å…¨éƒ¨ Map å®Œæˆå±æ­£å¸¸ |
| ä»»åŠ¡å¡ä½ | æŸ¥çœ‹ NodeManager æ—¥å¿—ã€ç£ç›˜æ˜¯å¦æ»¡ |

---

### ğŸ§© Git ä½¿ç”¨é€Ÿè§ˆ
```bash
git init
git add .
git commit -m "Initial commit"
git remote add origin git@github.com:yourname/hadoop-mr-experiment.git
git push -u origin master
```

---
### é‡å¯åhadoopå¯åŠ¨

åœ¨hadoop001:

start-dfs.sh

ssh hadoop002

start-yarn.sh

exit

## hadoopé¢æ¿æŸ¥çœ‹

hadoop001:9870 æ˜¯æ–‡ä»¶ç®¡ç†ç³»ç»Ÿçš„é¢æ¿

hadoop002:8088 æ˜¯åˆ†å¸ƒå¼ä»»åŠ¡é¢æ¿

## ğŸ›  å¸¸è§é—®é¢˜
### å¤§æ•°æ®å´©æºƒé—®é¢˜
cd /opt/hadoop/etc/hadoop  è¿›å…¥é…ç½®æ–‡ä»¶ç›®å½•ä¸‹

åœ¨ mapred-site.xml åŠ å…¥
'''
<!-- Map ä»»åŠ¡å†…å­˜ï¼ˆæ ¹æ®æœ€å¼±èŠ‚ç‚¹ 4G è§„åˆ’ï¼‰-->
    <property>
        <name>mapreduce.map.memory.mb</name>
        <value>1024</value>
    </property>
    <property>
        <name>mapreduce.map.java.opts</name>
        <value>-Xmx820m</value>
    </property>

    <!-- Reduce ä»»åŠ¡å†…å­˜ï¼ˆçº¦ 2Gï¼‰ -->
    <property>
        <name>mapreduce.reduce.memory.mb</name>
        <value>2048</value>
    </property>
    <property>
        <name>mapreduce.reduce.java.opts</name>
        <value>-Xmx1640m</value>
    </property>

    <!-- Shuffle IO -->
    <property>
        <name>mapreduce.task.io.sort.mb</name>
        <value>256</value>
    </property>

    <!-- æ¯ä¸ªèŠ‚ç‚¹å¹¶è¡Œä»»åŠ¡æ•°é‡ï¼ˆç”± CPU å†³å®šï¼‰ -->
    <property>
        <name>mapreduce.tasktracker.map.tasks.maximum</name>
        <value>3</value>
    </property>
    <property>
        <name>mapreduce.tasktracker.reduce.tasks.maximum</name>
        <value>2</value>
    </property>
'''

åœ¨ yarn-site.xml åŠ å…¥
'''
<!-- ä»¥ä¸‹æ–°å¢ -->
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>6144</value> <!-- ç»™ 4C8G èŠ‚ç‚¹ -->
    </property>

    <!-- CPU Core æ•° -->
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>4</value>
    </property>

    <!-- Container æœ€å°å’Œæœ€å¤§å†…å­˜ -->
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>512</value>
    </property>

    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>4096</value>
    </property>
'''

---

## ğŸ“ 100MBæ•°æ®å®éªŒæ“ä½œè®°å½• (2025-11-25)

### å®éªŒç›®æ ‡
ç”Ÿæˆ100MBæµ‹è¯•æ•°æ®å¹¶è¿è¡Œä¸€æ¬¡å®Œæ•´çš„MapReduceå®éªŒï¼ŒéªŒè¯ç³»ç»Ÿé…ç½®å’Œæ€§èƒ½ç›‘æµ‹åŠŸèƒ½ã€‚

### é‡åˆ°çš„é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

#### é—®é¢˜1ï¼šTimeouté™åˆ¶å¯¼è‡´ä½œä¸šè¢«æ€
**ç°è±¡**: 
- ä½œä¸šè¿è¡Œæ—¶è¢«timeoutå‘½ä»¤å¼ºåˆ¶ç»ˆæ­¢
- YARNæ˜¾ç¤ºä½œä¸šçŠ¶æ€ä¸ºKILLED
- å®éªŒCSVæ˜¾ç¤ºjob_statusä¸ºFAILED

**åŸå› **: 
`scripts/monitor_job.sh` ç¬¬70è¡Œä½¿ç”¨äº† `timeout 300` å‘½ä»¤ï¼Œé™åˆ¶ä½œä¸šæœ€å¤šè¿è¡Œ300ç§’ï¼ˆ5åˆ†é’Ÿï¼‰

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ä¿®æ”¹ scripts/monitor_job.sh
# å°†ç¬¬70è¡Œä»ï¼š
timeout 300 hadoop jar target/reduce-startup-1.0-SNAPSHOT-jar-with-dependencies.jar "${INPUT_PATH}" "${OUTPUT_PATH}" 2>&1 | tee "${JOB_LOG_FILE}"

# æ”¹ä¸ºï¼š
hadoop jar target/reduce-startup-1.0-SNAPSHOT-jar-with-dependencies.jar "${INPUT_PATH}" "${OUTPUT_PATH}" 2>&1 | tee "${JOB_LOG_FILE}"
```

#### é—®é¢˜2ï¼šReduceä»»åŠ¡å†…å­˜éœ€æ±‚è¶…è¿‡é›†ç¾¤é™åˆ¶
**ç°è±¡**:
```
REDUCE capability required is more than the supported max container capability in the cluster.
reduceResourceRequest: <memory:6096, vCores:1>
maxContainerCapability:<memory:4096, vCores:4>
```

**åŸå› **: 
- Hadoopé»˜è®¤Reduceä»»åŠ¡éœ€è¦6096MBå†…å­˜
- ä½†é›†ç¾¤é…ç½®çš„å•å®¹å™¨æœ€å¤§å†…å­˜åªæœ‰4096MB
- å¯¼è‡´YARNæ‹’ç»åˆ†é…èµ„æºï¼Œä½œä¸šè¢«KILLED

**è§£å†³æ–¹æ¡ˆ**:
åœ¨ `src/main/java/edu/example/mapreduce/Main.java` ä¸­æ·»åŠ å†…å­˜é…ç½®ï¼š

```java
Configuration conf = new Configuration();

// â­ å®éªŒ Aï¼šæ§åˆ¶ Reduce å¯åŠ¨æ—¶æœº
conf.setFloat("mapreduce.job.reduce.slowstart.completedmaps", 0.3f);

// è®¾ç½®å†…å­˜é…ç½®ï¼Œç¡®ä¿ä¸è¶…è¿‡é›†ç¾¤é™åˆ¶ï¼ˆæœ€å¤§4096MBï¼‰
conf.set("mapreduce.map.memory.mb", "2048");
conf.set("mapreduce.reduce.memory.mb", "3072");
conf.set("mapreduce.map.java.opts", "-Xmx1638m");
conf.set("mapreduce.reduce.java.opts", "-Xmx2458m");
```

### å®Œæ•´æ“ä½œæ­¥éª¤

#### 1. ç”Ÿæˆ100MBæµ‹è¯•æ•°æ®
```bash
# ä½¿ç”¨æ•°æ®ç”Ÿæˆè„šæœ¬
python3 scripts/generate_data.py 100 --output input-100mb --prefix data

# ç”Ÿæˆç»“æœï¼š
# - 8ä¸ªæ–‡ä»¶ï¼ˆdata01.txt ~ data08.txtï¼‰
# - æ¯ä¸ªæ–‡ä»¶çº¦12.5MB
# - æ€»è®¡çº¦100MB
# - è‡ªåŠ¨ç”Ÿæˆä¸Šä¼ è„šæœ¬ upload_to_hdfs.sh
```

#### 2. ä¸Šä¼ æ•°æ®åˆ°HDFS
```bash
cd input-100mb
./upload_to_hdfs.sh /mr_input_100mb_20251125

# éªŒè¯ä¸Šä¼ 
hdfs dfs -ls /mr_input_100mb_20251125
hdfs dfs -du -h /mr_input_100mb_20251125
```

#### 3. ä¿®æ”¹é…ç½®æ–‡ä»¶
```bash
# 1. ä¿®æ”¹ scripts/monitor_job.sh ç§»é™¤timeouté™åˆ¶
# 2. ä¿®æ”¹ src/main/java/edu/example/mapreduce/Main.java æ·»åŠ å†…å­˜é…ç½®
```

#### 4. è¿è¡Œå®éªŒ
```bash
# è¿è¡Œå•æ¬¡å®éªŒ
./scripts/monitor_job.sh 0.3 /mr_input_100mb_20251125 /mr_output_100mb_20251125_success

# å®éªŒID: 20251125_131923
# Slowstartå€¼: 0.3
```

### å®éªŒç»“æœ

#### ä½œä¸šç»Ÿè®¡ä¿¡æ¯
```
Job Status: SUCCESS âœ“
Total Time: 47 seconds
Input Data: 104,857,846 bytes (çº¦100MB)
Output Data: 11,731,250 bytes (çº¦11.2MB)
Map Tasks: 9 (8 successful + 1 killed)
Reduce Tasks: 1
```

#### è¯¦ç»†æ€§èƒ½æŒ‡æ ‡
```
Mapé˜¶æ®µ:
- Mapè¾“å…¥è®°å½•: 1,226,345
- Mapè¾“å‡ºè®°å½•: 13,006,924
- Mapè¾“å‡ºå­—èŠ‚: 156,885,542
- æœ¬åœ°Mapä»»åŠ¡æ•°: 9
- Mapæ€»è€—æ—¶: 84,289 ms

Reduceé˜¶æ®µ:
- Reduceè¾“å…¥ç»„æ•°: 1,056,294
- Reduceè¾“å…¥è®°å½•: 13,006,924
- Reduceè¾“å‡ºè®°å½•: 1,056,294
- Shuffleå­—èŠ‚æ•°: 182,899,438
- Reduceæ€»è€—æ—¶: 14,112 ms

èµ„æºä½¿ç”¨:
- CPUæ—¶é—´: 62,020 ms
- GCæ—¶é—´: 3,243 ms
- ç‰©ç†å†…å­˜å³°å€¼: 636,633,088 bytes (Map)
- ç‰©ç†å†…å­˜å³°å€¼: 516,087,808 bytes (Reduce)
```

#### ç³»ç»Ÿç›‘æ§æŒ‡æ ‡
```
å®éªŒæœŸé—´ç»Ÿè®¡:
- å¹³å‡CPUä½¿ç”¨ç‡: 49.98%
- æœ€å¤§CPUä½¿ç”¨ç‡: 100.00%
- å¹³å‡å†…å­˜ä½¿ç”¨: 49.98 MB
- æœ€å¤§å†…å­˜ä½¿ç”¨: 100.00 MB
- å¹³å‡è´Ÿè½½: 43.39
- æœ€å¤§è´Ÿè½½: 62.90
```

#### HDFSè¾“å‡ºéªŒè¯
```bash
$ hdfs dfs -ls /mr_output_100mb_20251125_success
Found 2 items
-rw-r--r--   3 ecs-user supergroup          0 2025-11-25 13:20 /mr_output_100mb_20251125_success/_SUCCESS
-rw-r--r--   3 ecs-user supergroup   11731250 2025-11-25 13:20 /mr_output_100mb_20251125_success/part-r-00000
```

### å®éªŒç»“è®º

1. **æˆåŠŸå®Œæˆ**: åœ¨è§£å†³timeoutå’Œå†…å­˜é…ç½®é—®é¢˜åï¼Œ100MBæ•°æ®å®éªŒæˆåŠŸå®Œæˆ
2. **æ‰§è¡Œæ•ˆç‡**: 47ç§’å¤„ç†100MBæ•°æ®ï¼Œæ€§èƒ½è¡¨ç°è‰¯å¥½
3. **é…ç½®ä¼˜åŒ–**: è¯æ˜äº†å†…å­˜é…ç½®çš„é‡è¦æ€§ï¼Œéœ€è¦æ ¹æ®é›†ç¾¤å®é™…èµ„æºé™åˆ¶è¿›è¡Œè°ƒæ•´
4. **ç›‘æ§ç³»ç»Ÿ**: CSVè®°å½•å’Œç³»ç»Ÿç›‘æ§åŠŸèƒ½æ­£å¸¸å·¥ä½œï¼Œæ•°æ®å®Œæ•´

### åç»­å»ºè®®

1. **æ›´å¤§æ•°æ®é›†**: å¯ä»¥å°è¯•500MBæˆ–1GBæ•°æ®é›†æµ‹è¯•
2. **å‚æ•°è°ƒä¼˜**: å¯ä»¥æµ‹è¯•ä¸åŒslowstartå€¼ï¼ˆ0.1, 0.5, 0.7, 1.0ï¼‰å¯¹æ¯”æ€§èƒ½
3. **æ‰¹é‡å®éªŒ**: ä½¿ç”¨ `batch_experiment.sh` è¿›è¡Œå¤šç»„å¯¹æ¯”å®éªŒ
4. **å†…å­˜ç›‘æ§**: å…³æ³¨ä¸åŒæ•°æ®è§„æ¨¡ä¸‹çš„å†…å­˜ä½¿ç”¨æƒ…å†µ

### å…³é”®æ–‡ä»¶ä½ç½®
- å®éªŒç»“æœCSV: `metrics/experiment_20251125_131923_slowstart_0.3.csv`
- ç³»ç»ŸæŒ‡æ ‡CSV: `iZbp17ue5tnwdnupp4di68Z.csv`
- HDFSè¾“å…¥ç›®å½•: `/mr_input_100mb_20251125`
- HDFSè¾“å‡ºç›®å½•: `/mr_output_100mb_20251125_success`

---

## ğŸ”„ ç³»ç»ŸæŒ‡æ ‡æ–‡ä»¶ç®¡ç†ä¼˜åŒ– (2025-11-25)

### é—®é¢˜æè¿°
ä¹‹å‰çš„ç³»ç»ŸæŒ‡æ ‡CSVæ–‡ä»¶ä½¿ç”¨å›ºå®šæ–‡ä»¶åï¼ˆå¦‚`iZbp17ue5tnwdnupp4di68Z.csv`ï¼‰ï¼Œå¯¼è‡´æ¯æ¬¡å®éªŒéƒ½ä¼šè¦†ç›–ä¸Šä¸€æ¬¡çš„æ•°æ®ï¼Œæ— æ³•ä¿ç•™å†å²è®°å½•ã€‚

### è§£å†³æ–¹æ¡ˆ

#### 1. æ–°çš„æ–‡ä»¶å‘½åæ ¼å¼
```
system_metrics/{èŠ‚ç‚¹å}_{æ—¶é—´æˆ³}.csv
```

**ç¤ºä¾‹**ï¼š
- `system_metrics/iZbp17ue5tnwdnupp4di68Z_20251125_133131.csv`
- `system_metrics/master_20251125_140000.csv`
- `system_metrics/worker01_20251125_140000.csv`

#### 2. ä¿®æ”¹çš„è„šæœ¬

**scripts/collect_metrics.sh**ï¼š
- æ·»åŠ æ—¶é—´æˆ³ç”Ÿæˆï¼š`TIMESTAMP=$(date +%Y%m%d_%H%M%S)`
- ä¿®æ”¹è¾“å‡ºè·¯å¾„ï¼š`OUTPUT_FILE="system_metrics/${NODE_NAME}_${TIMESTAMP}.csv"`
- è‡ªåŠ¨åˆ›å»ºç›®å½•ï¼š`mkdir -p system_metrics`

**scripts/monitor_job.sh**ï¼š
- åŒæ­¥æ—¶é—´æˆ³ç”Ÿæˆ
- æ›´æ–°ç³»ç»ŸæŒ‡æ ‡æ–‡ä»¶è·¯å¾„ï¼š`SYSTEM_METRICS_FILE="system_metrics/${NODE_NAME}_${TIMESTAMP}.csv"`

#### 3. æ–°å¢æ–‡ä»¶
- **ç›®å½•**ï¼š`system_metrics/` - ä¸“é—¨å­˜å‚¨ç³»ç»Ÿæ—¶åºæŒ‡æ ‡æ•°æ®
- **æ–‡æ¡£**ï¼š`system_metrics/README.md` - è¯¦ç»†è¯´æ˜æ–‡ä»¶æ ¼å¼å’Œä½¿ç”¨æ–¹æ³•
- **.gitignore**ï¼šæ·»åŠ  `system_metrics/` æ’é™¤è§„åˆ™

### ä¼˜åŠ¿

1. **æ•°æ®ä¿ç•™**ï¼šæ¯æ¬¡å®éªŒçš„ç³»ç»ŸæŒ‡æ ‡ç‹¬ç«‹ä¿å­˜ï¼Œä¸ä¼šè¦†ç›–
2. **å¯è¿½æº¯**ï¼šé€šè¿‡æ—¶é—´æˆ³ç²¾ç¡®å®šä½åˆ°å…·ä½“å®éªŒ
3. **å¤šèŠ‚ç‚¹å‹å¥½**ï¼šæ–‡ä»¶ååŒ…å«èŠ‚ç‚¹åï¼Œä¾¿äºåŒºåˆ†
4. **ä¾¿äºåˆ†æ**ï¼šå¯ä»¥å¯¹æ¯”å¤šæ¬¡å®éªŒçš„ç³»ç»Ÿèµ„æºä½¿ç”¨è¶‹åŠ¿

### ä½¿ç”¨ç¤ºä¾‹

```bash
# è¿è¡Œå®éªŒåï¼ŒæŸ¥çœ‹ç”Ÿæˆçš„ç³»ç»ŸæŒ‡æ ‡æ–‡ä»¶
ls -lht system_metrics/

# æŸ¥çœ‹ç‰¹å®šèŠ‚ç‚¹åœ¨æŸä¸ªæ—¶é—´æ®µçš„æ‰€æœ‰å®éªŒ
ls -lh system_metrics/iZbp17ue5tnwdnupp4di68Z_202511*

# åˆ†ææŸæ¬¡å®éªŒçš„ç³»ç»ŸæŒ‡æ ‡
cat system_metrics/iZbp17ue5tnwdnupp4di68Z_20251125_133131.csv
```

### æ•°æ®æ¸…ç†

å®šæœŸæ¸…ç†æ—§æ•°æ®ä»¥èŠ‚çœç©ºé—´ï¼š

```bash
# åˆ é™¤30å¤©å‰çš„æ•°æ®
find system_metrics/ -name "*.csv" -mtime +30 -delete

# åªä¿ç•™æœ€è¿‘10æ¬¡å®éªŒ
ls -t system_metrics/*.csv | tail -n +11 | xargs rm -f
```

---

## ğŸ• Map/Reduceæ—¶é—´çº¿æå–å·¥å…· (2025-11-25)

### åŠŸèƒ½è¯´æ˜

ä¸ºäº†æ·±å…¥åˆ†æMapå’ŒReduceä»»åŠ¡çš„å¹¶è¡Œæ‰§è¡Œæƒ…å†µï¼Œé¡¹ç›®æä¾›äº†æ—¶é—´çº¿æå–å·¥å…·`extract_timeline.sh`ã€‚è¯¥å·¥å…·é€šè¿‡Hadoop JobHistory Serverçš„REST APIï¼Œè‡ªåŠ¨æå–æ¯ä¸ªä»»åŠ¡çš„å¯åŠ¨å’Œå®Œæˆæ—¶é—´ï¼Œç”Ÿæˆç‹¬ç«‹çš„timeline CSVæ–‡ä»¶ã€‚

### ğŸ“Š ç”Ÿæˆçš„æ–‡ä»¶

#### 1. ä»»åŠ¡æ—¶é—´çº¿è¯¦ç»†æ•°æ®
**æ–‡ä»¶å**: `metrics/{å®éªŒID}_slowstart_{å€¼}_timeline.csv`

è®°å½•æ¯ä¸ªMap/Reduceä»»åŠ¡çš„åŸå§‹æ—¶é—´æ•°æ®ï¼š

| åˆ—å | è¯´æ˜ |
|------|------|
| `experiment_id` | å®éªŒæ ‡è¯†ç¬¦ |
| `slowstart_value` | æ…¢å¯åŠ¨å‚æ•°å€¼ |
| `task_id` | ä»»åŠ¡IDï¼ˆå¦‚task_xxx_m_000001ï¼‰ |
| `task_type` | ä»»åŠ¡ç±»å‹ï¼ˆMAP/REDUCEï¼‰ |
| `start_time` | ä»»åŠ¡å¯åŠ¨æ—¶é—´ï¼ˆUnixæ—¶é—´æˆ³ï¼‰ |
| `finish_time` | ä»»åŠ¡å®Œæˆæ—¶é—´ï¼ˆUnixæ—¶é—´æˆ³ï¼‰ |
| `elapsed_sec` | ä»»åŠ¡æ‰§è¡Œæ—¶é•¿ï¼ˆç§’ï¼‰ |
| `shuffle_finish_time` | Reduceçš„Shuffleé˜¶æ®µå®Œæˆæ—¶é—´ |
| `merge_finish_time` | Reduceçš„Mergeé˜¶æ®µå®Œæˆæ—¶é—´ |
| `reduce_finish_time` | Reduceçš„è®¡ç®—é˜¶æ®µå®Œæˆæ—¶é—´ |

#### 2. æ—¶é—´çº¿ç»Ÿè®¡æ±‡æ€»
**æ–‡ä»¶å**: `metrics/{å®éªŒID}_slowstart_{å€¼}_timeline_summary.csv`

ç»Ÿè®¡Map/Reduceçš„å¹¶è¡Œæ‰§è¡Œæƒ…å†µï¼š

| åˆ—å | è¯´æ˜ |
|------|------|
| `experiment_id` | å®éªŒæ ‡è¯†ç¬¦ |
| `slowstart_value` | æ…¢å¯åŠ¨å‚æ•°å€¼ |
| `num_map_tasks` | Mapä»»åŠ¡æ€»æ•° |
| `num_reduce_tasks` | Reduceä»»åŠ¡æ€»æ•° |
| `map_start_time` | æœ€æ—©Mapä»»åŠ¡å¯åŠ¨æ—¶é—´ |
| `map_end_time` | æœ€æ™šMapä»»åŠ¡å®Œæˆæ—¶é—´ |
| `map_duration_sec` | Mapé˜¶æ®µæ€»æ—¶é•¿ |
| `reduce_start_time` | æœ€æ—©Reduceä»»åŠ¡å¯åŠ¨æ—¶é—´ |
| `reduce_end_time` | æœ€æ™šReduceä»»åŠ¡å®Œæˆæ—¶é—´ |
| `reduce_duration_sec` | Reduceé˜¶æ®µæ€»æ—¶é•¿ |
| `overlap_duration_sec` | **Map/Reduceé‡å æ‰§è¡Œæ—¶é—´** â­ |
| `reduce_start_at_map_pct` | **Reduceåœ¨Mapæ‰§è¡Œç™¾åˆ†æ¯”æ—¶å¯åŠ¨** â­ |
| `total_time_sec` | ä½œä¸šæ€»æ‰§è¡Œæ—¶é—´ |
| `time_saved_sec` | é€šè¿‡å¹¶è¡Œæ‰§è¡ŒèŠ‚çœçš„æ—¶é—´ |
| `parallel_efficiency_pct` | å¹¶è¡Œæ•ˆç‡ç™¾åˆ†æ¯” |

### ğŸš€ ä½¿ç”¨æ–¹æ³•

#### è‡ªåŠ¨æå–ï¼ˆæ¨èï¼‰

è¿è¡Œ`monitor_job.sh`æ—¶ä¼šè‡ªåŠ¨æå–timelineæ•°æ®ï¼š

```bash
# è¿è¡Œå®éªŒ
./scripts/monitor_job.sh 0.3 /mr_input /mr_output

# å®éªŒå®Œæˆåè‡ªåŠ¨ç”Ÿæˆä¸‰ä¸ªæ–‡ä»¶ï¼š
# 1. metrics/experiment_{ID}_slowstart_0.3.csv       (å®éªŒç»“æœ)
# 2. metrics/{ID}_slowstart_0.3_timeline.csv         (ä»»åŠ¡æ—¶é—´çº¿)
# 3. metrics/{ID}_slowstart_0.3_timeline_summary.csv (æ—¶é—´çº¿ç»Ÿè®¡)
```

#### æ‰‹åŠ¨æå–

å¯¹äºå·²å®Œæˆçš„å®éªŒï¼Œå¯ä»¥æ‰‹åŠ¨æå–timelineï¼š

```bash
# åŸºæœ¬ç”¨æ³•
./scripts/extract_timeline.sh application_1764041163594_0018 0.3

# æŒ‡å®šå®éªŒID
./scripts/extract_timeline.sh application_1764041163594_0018 0.3 20251125_141801

# æŸ¥æ‰¾application IDçš„æ–¹æ³•
yarn application -list -appStates FINISHED | tail -5
```

### ğŸ“‹ å‰ç½®è¦æ±‚

**å¿…é¡»å¯åŠ¨JobHistory Server**ï¼Œå¦åˆ™æ— æ³•é€šè¿‡REST APIè·å–æ•°æ®ï¼š

```bash
# åœ¨hadoop001ä¸Šå¯åŠ¨
mapred --daemon start historyserver

# éªŒè¯å¯åŠ¨
jps | grep JobHistoryServer

# è®¿é—®Webç•Œé¢
http://hadoop001:19888
```

### ğŸ” å®é™…æ¡ˆä¾‹ï¼š500MBæ•°æ® slowstart=0.3

#### Timelineè¯¦ç»†æ•°æ®ç¤ºä¾‹
```csv
experiment_id,slowstart_value,task_id,task_type,start_time,finish_time,elapsed_sec
20251125_141801,0.3,task_..._m_000000,MAP,1764051516,1764051546,30
20251125_141801,0.3,task_..._m_000001,MAP,1764051516,1764051546,29
...
20251125_141801,0.3,task_..._r_000000,REDUCE,1764051547,1764051607,59
```

#### Timelineæ±‡æ€»ç»Ÿè®¡
```csv
experiment_id: 20251125_141801
slowstart_value: 0.3
num_map_tasks: 8
num_reduce_tasks: 1
map_duration_sec: 47
reduce_duration_sec: 60
overlap_duration_sec: 16        â­ Map/Reduceé‡å 16ç§’
reduce_start_at_map_pct: 65.96  â­ Reduceåœ¨Mapæ‰§è¡Œ66%æ—¶å¯åŠ¨
parallel_efficiency_pct: 14.95   â­ èŠ‚çœäº†15%çš„æ—¶é—´
```

### ğŸ’¡ æ•°æ®è§£è¯»

ä»500MBå®éªŒçš„timelineæ•°æ®å¯ä»¥çœ‹å‡ºï¼š

1. **å¹¶è¡Œæ‰§è¡Œç¡®è®¤**ï¼š
   - Mapé˜¶æ®µï¼š14:18:36 â†’ 14:19:23 (47ç§’)
   - Reduceå¯åŠ¨ï¼š14:19:07ï¼ˆMapæ‰§è¡Œåˆ°66%æ—¶ï¼‰
   - **é‡å æ—¶é—´ï¼š16ç§’**

2. **Slowstartæ•ˆæœ**ï¼š
   - è™½ç„¶è®¾ç½®ä¸º0.3ï¼Œä½†Reduceåœ¨66%æ—¶æ‰å¯åŠ¨
   - è¿™æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºslowstartåŸºäº**æ•°æ®å¤„ç†è¿›åº¦**è€Œéæ—¶é—´
   - å‰æœŸMapå¤„ç†å¿«é€Ÿæ•°æ®ï¼ŒåæœŸå¤„ç†å¤æ‚æ•°æ®

3. **æ€§èƒ½æå‡**ï¼š
   - ç†è®ºä¸²è¡Œæ—¶é—´ï¼š47 + 60 = 107ç§’
   - å®é™…æ‰§è¡Œæ—¶é—´ï¼š91ç§’
   - **èŠ‚çœ16ç§’ï¼ˆ15%ï¼‰**

### ğŸ¯ å¯¹æ¯”ä¸åŒslowstartå€¼

é€šè¿‡timelineå¯ä»¥æ¸…æ¥šå¯¹æ¯”ä¸åŒå‚æ•°çš„æ•ˆæœï¼š

| Slowstart | Reduceå¯åŠ¨æ—¶æœº | é‡å æ—¶é—´ | å¹¶è¡Œæ•ˆç‡ |
|-----------|----------------|----------|----------|
| 0.1 | Map 10-20%æ—¶ | æ›´é•¿ | å¯èƒ½ç©ºè½¬ |
| 0.3 | Map 30-40%æ—¶ | é€‚ä¸­ | è¾ƒä¼˜ â­ |
| 0.7 | Map 70-80%æ—¶ | è¾ƒçŸ­ | åä¸²è¡Œ |
| 1.0 | Map 100%å | æ—  | å®Œå…¨ä¸²è¡Œ |

### ğŸ› ï¸ æ•…éšœæ’é™¤

**é—®é¢˜ï¼šJobHistory Serverè¿æ¥å¤±è´¥**

```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
jps | grep JobHistoryServer

# å¦‚æœæ²¡æœ‰è¿è¡Œï¼Œå¯åŠ¨å®ƒ
mapred --daemon start historyserver

# æ£€æŸ¥ç«¯å£
